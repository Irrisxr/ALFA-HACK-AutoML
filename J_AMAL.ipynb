{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import gc\n",
        "from numba import jit"
      ],
      "metadata": {
        "id": "UazhfXeUb4yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для кагла\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],  # Вывод в stdout\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")"
      ],
      "metadata": {
        "id": "WC4mx1I5b9BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, random_seed):\n",
        "        self.random_state = random_seed\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "        self.target = None\n",
        "        self.cat_features = None\n",
        "\n",
        "    def read_data(self, train_path: str, test_path: str) -> None:\n",
        "        '''Создается train, test, target для train'''\n",
        "        logging.info('\\nНачало предобработки')\n",
        "        self.train = pd.read_parquet(train_path)\n",
        "        self.test = pd.read_parquet(test_path)\n",
        "        self.target = self.train['target']\n",
        "        self.train.drop(['target', 'smpl'], axis=1, inplace = True)\n",
        "        self.test.drop(['smpl'], axis=1, inplace = True)\n",
        "        logging.info('Данные прочитаны и разделены')\n",
        "\n",
        "    def type_research(self) -> None:\n",
        "        self.cat_features = []\n",
        "        for i in self.train.columns:\n",
        "            if self.train[i].dropna().nunique()/len(self.train[i].dropna()) < 0.5:\n",
        "                self.cat_features.append(i)\n",
        "        logging.info('Типы определены')\n",
        "\n",
        "    def filling_nans(self) -> None:\n",
        "        '''...'''\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('string')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('string')\n",
        "        logging.info('Пропущенные значения заполнены')\n",
        "\n",
        "\n",
        "    def select_features_with_catboost(self) -> None:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.train, self.target, test_size=0.2, random_state=self.random_state, stratify=self.target)\n",
        "        params = {\n",
        "            'iterations': 3000,\n",
        "            'early_stopping_rounds': 150,\n",
        "            'loss_function': 'CrossEntropy',\n",
        "            'l2_leaf_reg': 4.5,\n",
        "            'task_type': 'GPU',\n",
        "            'cat_features': self.cat_features,\n",
        "            'random_seed': self.random_state,\n",
        "            'grow_policy': 'Lossguide',\n",
        "            'eval_metric': 'AUC',\n",
        "            'depth': 7\n",
        "        }\n",
        "        if len(X_train) < 100000:\n",
        "            fractions = [0.4, 0.3, 0.2]\n",
        "            fractions = [0.4]\n",
        "        else:\n",
        "            fractions = [0.4]\n",
        "        best_score = 0\n",
        "        last_features = None\n",
        "        for _ in fractions:\n",
        "            model = CatBoostClassifier(**params)\n",
        "            summary = model.select_features(X_train, y_train,\n",
        "                              eval_set=(X_test, y_test),\n",
        "                              features_for_select=X_train.columns,\n",
        "                              num_features_to_select=round(len(X_train.columns) * _),\n",
        "                              steps=3,\n",
        "                              algorithm='RecursiveByPredictionValuesChange',\n",
        "                              train_final_model=True,\n",
        "                              logging_level='Silent')\n",
        "            if model.get_best_score()['validation']['AUC'] > best_score:\n",
        "                best_score = model.get_best_score()['validation']['AUC']\n",
        "                last_features = summary['selected_features_names']\n",
        "\n",
        "        del X_train, X_test, y_train, y_test\n",
        "        gc.collect()\n",
        "        self.train = self.train[last_features]\n",
        "        self.test = self.test[last_features]\n",
        "        self.cat_features = list(set(last_features) & set(self.cat_features))\n",
        "        logging.info('Признаки отобраны')\n",
        "\n",
        "    def remove_correlated_features(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def normalize_data(self) -> None:\n",
        "        numeric_columns = self.train.select_dtypes(include=[np.number]).columns\n",
        "        if not numeric_columns.empty:\n",
        "            scaler = StandardScaler()\n",
        "            columns_to_scale = self.train.select_dtypes(include=[np.number]).columns\n",
        "            self.train[columns_to_scale] = scaler.fit_transform(self.train[columns_to_scale])\n",
        "            self.test[columns_to_scale] = scaler.transform(self.test[columns_to_scale])\n",
        "            logging.info('Числовые столбцы нормализованны')\n",
        "        else:\n",
        "            logging.info('Нормализация не потребовалась')\n",
        "\n",
        "    def preprocess_all_data(self, train_path: str, test_path: str):\n",
        "        self.read_data(train_path, test_path)\n",
        "        self.type_research()\n",
        "        self.filling_nans()\n",
        "        self.remove_correlated_features()\n",
        "        self.select_features_with_catboost()\n",
        "        self.normalize_data()\n",
        "        return self.train, self.test, self.target, self.cat_features\n"
      ],
      "metadata": {
        "id": "PyHgzbH0cii9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, cat_features, random_seed):\n",
        "        self.cat_features = cat_features\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def predict(self, X_test) -> np.ndarray:\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "    def predict_proba(self, X_test) -> np.ndarray:\n",
        "        return self.model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "class CB_model(Model):\n",
        "    def __init__(self, cat_features: str, random_seed: int):\n",
        "        super().__init__(cat_features, random_seed)\n",
        "        self.params = {'iterations': 3000,\n",
        "          'task_type': 'GPU',\n",
        "          'depth': 7,\n",
        "          'early_stopping_rounds': 150,\n",
        "          'grow_policy' : 'Lossguide',\n",
        "          'l2_leaf_reg': 4.5,\n",
        "          'eval_metric': 'AUC',\n",
        "          'bagging_temperature': .5,\n",
        "          'bootstrap_type': 'Bayesian',\n",
        "          'loss_function': 'CrossEntropy',\n",
        "          'random_strength': 20,\n",
        "          'verbose' : False,\n",
        "          'random_state' : self.random_seed,\n",
        "         }\n",
        "\n",
        "    def train_with_validation(self, X_train, y_train, X_test, y_test) -> None:\n",
        "        logging.info('Начало обучения CatBoost')\n",
        "\n",
        "        self.model = CatBoostClassifier(**self.params)\n",
        "        self.model.fit(X_train, y_train, cat_features = self.cat_features, eval_set = (X_test, y_test))\n",
        "        self.params['iterations'] = self.model.get_best_iteration() + 200\n",
        "        logging.info('CatBoost обучен на валидации')\n",
        "\n",
        "class XGB_model(Model):\n",
        "    def __init__(self, cat_features: str, random_seed: int):\n",
        "        super().__init__(cat_features, random_seed)\n",
        "        self.params = {\n",
        "                'n_estimators': 3000,\n",
        "                'learning_rate':0.03,\n",
        "                'max_depth': 7,\n",
        "                'use_label_encoder':False,\n",
        "                'enable_categorical':True,\n",
        "                'eval_metric':'auc',\n",
        "                'tree_method':'hist',\n",
        "                'device':'cuda',\n",
        "                'reg_lambda': 8,\n",
        "                'reg_alpha': 20,\n",
        "                'max_bin': 1000,\n",
        "                'subsample': 0.8,\n",
        "                'grow_policy': 'lossguide',\n",
        "                'min_child_weight' : 5,\n",
        "                'sampling_method': 'uniform',\n",
        "                'early_stopping_rounds': 100,\n",
        "                'random_state': self.random_seed,\n",
        "                'colsample_bytree' : 0.8,\n",
        "            }\n",
        "\n",
        "\n",
        "    def train_with_validation(self, X_train, y_train, X_test, y_test) -> None:\n",
        "        X_train[self.cat_features] = X_train[self.cat_features].astype('category')\n",
        "        X_test[self.cat_features] = X_test[self.cat_features].astype('category')\n",
        "\n",
        "        self.model = XGBClassifier(**self.params)\n",
        "        self.model.fit(X_train, y_train,\n",
        "                       eval_set = [(X_test, y_test)],\n",
        "                      verbose=False)\n",
        "        self.params['n_estimators'] = self.model.best_iteration + 200\n",
        "        del self.params['early_stopping_rounds'], self.params['eval_metric'] #\n",
        "\n",
        "        logging.info('XGB обучен на валидации')\n",
        "\n",
        "class Model_training:\n",
        "    def __init__(self, random_seed):\n",
        "        self.random_seed = random_seed\n",
        "        self.cb1 = None\n",
        "        self.xgb1 = None\n",
        "        self.cb2 = None\n",
        "        self.xgb2 = None\n",
        "\n",
        "    def train_and_tune_models(self, train, target, cat_features) -> None:\n",
        "        logging.info('\\nНачало обучения моделей на валидации')\n",
        "        self.cb1 = CB_model(cat_features, self.random_seed)\n",
        "        self.xgb1 = XGB_model(cat_features, self.random_seed)\n",
        "        self.cb2 = CB_model(cat_features, self.random_seed)\n",
        "        self.xgb2 = XGB_model(cat_features, self.random_seed)\n",
        "        self.cb1.params['depth'] = 6\n",
        "        if len(train) > 700000:\n",
        "            self.xgb1.params['max_depth'] = 8\n",
        "        else:\n",
        "            self.xgb1.params['max_depth'] = 6\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=self.random_seed, stratify=target)\n",
        "        self.cb1.train_with_validation(X_train, y_train, X_test, y_test)\n",
        "        self.xgb1.train_with_validation(X_train, y_train, X_test, y_test)\n",
        "        self.cb2.train_with_validation(X_train, y_train, X_test, y_test)\n",
        "        self.xgb2.train_with_validation(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    def get_trained_models(self):\n",
        "        return {\n",
        "            'catboost_1': [self.cb1.model, self.cb1.params],\n",
        "            'xgboost_1': [self.xgb1.model, self.xgb1.params],\n",
        "            'catboost_2': [self.cb2.model, self.cb2.params],\n",
        "            'xgboost_2': [self.xgb2.model, self.xgb2.params],\n",
        "        }"
      ],
      "metadata": {
        "id": "GNNf95Uwcnzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ensemble:\n",
        "    def __init__(self, random_seed):\n",
        "        self.random_seed = random_seed\n",
        "        self.best_weights = None\n",
        "        self.metrics = None\n",
        "        self.models_rate = None\n",
        "        self.cb_blender_rs = None\n",
        "        self.blending = None\n",
        "\n",
        "    def calculate_roc_auc(self, models, train, target, cat_features):\n",
        "        logging.info('Начало обучения ансамбля')\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(train, target, stratify=target, test_size=0.2, random_state=self.random_seed)\n",
        "        roc_auc_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            if isinstance(model[0], CatBoostClassifier):\n",
        "                X_test[cat_features] = X_test[cat_features].astype('str')\n",
        "            else:\n",
        "                X_test[cat_features] = X_test[cat_features].astype('category')\n",
        "            y_pred_proba = model[0].predict_proba(X_test)[:, 1]\n",
        "            score = roc_auc_score(y_test, y_pred_proba)\n",
        "            roc_auc_scores[model_name] = score\n",
        "\n",
        "        sorted_models = sorted(roc_auc_scores.items(), key=lambda x: -x[1])\n",
        "        sorted_model_names, sorted_metrics = zip(*sorted_models)\n",
        "\n",
        "        self.models_rate = list(sorted_model_names)\n",
        "        self.metrics = list(sorted_metrics)\n",
        "        logging.info('ROC-AUC почитан, модели отсортированны')\n",
        "\n",
        "    def compute_weights(self, models, train, target, cat_features):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(train, target, stratify=target, test_size=0.2, random_state=self.random_seed)\n",
        "\n",
        "        preds = []\n",
        "        score = 0\n",
        "        for model in self.models_rate:\n",
        "            if isinstance(models[model][0], CatBoostClassifier):\n",
        "                X_test[cat_features] = X_test[cat_features].astype('str')\n",
        "            else:\n",
        "                X_test[cat_features] = X_test[cat_features].astype('category')\n",
        "            preds.append(models[model][0].predict_proba(X_test)[:, 1])\n",
        "        preds = np.array(preds)\n",
        "        for a in tqdm(np.arange(0.25, 1 + 0.00001, 0.05)):\n",
        "            for b in np.arange(0, 1 - a + 0.00001, 0.05):\n",
        "                for c in np.arange(0, 1 - a - b + 0.00001,0.05):\n",
        "                    cur = roc_auc_score(y_test, preds[0] * a + preds[1] * b + preds[2] * c + preds[3] * (1 - a - b - c))\n",
        "                    if cur > score:\n",
        "                        weights = np.array([a, b, c, 1 - a - b - c])\n",
        "                        score = cur\n",
        "\n",
        "\n",
        "        self.best_weights = weights\n",
        "        print(weights)\n",
        "        logging.info('Веса ансамбля посчитаны')\n",
        "\n",
        "\n",
        "    def train_blending_models(self, models, train, target, cat_features):\n",
        "        trained_models = []\n",
        "\n",
        "        for model_name in self.models_rate:\n",
        "            model = models[model_name][0].__class__()\n",
        "            print(models[model_name][1])\n",
        "            model.set_params(**models[model_name][1])\n",
        "\n",
        "            if isinstance(models[model_name][0], CatBoostClassifier):\n",
        "                train[cat_features] = train[cat_features].astype('str')\n",
        "\n",
        "            else:\n",
        "                train[cat_features] = train[cat_features].astype('category')\n",
        "\n",
        "            trained_model = model.fit(train, target)\n",
        "            trained_models.append(trained_model)\n",
        "\n",
        "        self.blending = trained_models\n",
        "        logging.info('Модели для ансамбля обучены')\n",
        "\n",
        "    def blending_predict(self, test, cat_features):\n",
        "        predictions = []\n",
        "        for model, weight in zip(self.blending, self.best_weights):\n",
        "            if isinstance(model, CatBoostClassifier):\n",
        "                test[cat_features] = test[cat_features].astype('str')\n",
        "                predictions.append(model.predict_proba(test)[:, 1] * weight)\n",
        "            else:\n",
        "                test[cat_features] = test[cat_features].astype('category')\n",
        "                predictions.append(model.predict_proba(test)[:, 1] * weight)\n",
        "        predictions = np.array(predictions)\n",
        "        return np.sum(predictions, axis = 0)\n",
        "        logging.info('Получено предсказание блендинга')\n",
        "\n",
        "    def stacking(self, train, target, test, models, cat_features):\n",
        "        blend_predictions = self.blending_predict(test, cat_features)\n",
        "\n",
        "        return blend_predictions\n",
        "        logging.info('Получено финальное предсказание')\n",
        "\n",
        "\n",
        "    def train_ensemble(self, train, test, target, cat_features, models):\n",
        "        self.calculate_roc_auc(models, train, target, cat_features)\n",
        "        self.compute_weights(models, train, target, cat_features)\n",
        "        self.train_blending_models(models, train, target, cat_features)\n",
        "        final_predictions = self.stacking(train, target, test, models, cat_features)\n",
        "        return final_predictions\n"
      ],
      "metadata": {
        "id": "70gwdGAHcw1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JAMAL:\n",
        "    def __init__(self, random_seed: int):\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def all_pipeline(self, train_path: str, test_path: str) -> pd.Series:\n",
        "        ### Весь preprocessing\n",
        "        preprocessor = Preprocessing(self.random_seed)\n",
        "        self.train, self.test, self.target, self.cat_features = preprocessor.preprocess_all_data(train_path, test_path)\n",
        "        del preprocessor\n",
        "        gc.collect()\n",
        "        ### Тюнинг\n",
        "        models = Model_training(self.random_seed)\n",
        "        # Трейнятся и тюнятся модели\n",
        "        models.train_and_tune_models(self.train, self.target, self.cat_features)\n",
        "        # Тюнится Ансамблирование\n",
        "        models_for_ensemble = models.get_trained_models()\n",
        "        ensemble = Ensemble(self.random_seed)\n",
        "        y_pred = ensemble.train_ensemble(self.train, self.test, self.target, self.cat_features, models_for_ensemble)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "Y8sBQ5MmczlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OrpTmkXby-Q"
      },
      "outputs": [],
      "source": [
        "def make_predict():\n",
        "    # Определяем пути к папкам 'data' и 'target_data'\n",
        "    data = 'data'\n",
        "    data_folder = os.listdir(data)\n",
        "\n",
        "    # Словарь для train и test файлов\n",
        "    train_test_files = {}\n",
        "\n",
        "    # Сканируем папку `data`\n",
        "    for dataset_name in data_folder:\n",
        "        dataset_path = data + f'/{dataset_name}'\n",
        "\n",
        "        if not os.path.isdir(dataset_path):\n",
        "            continue  # Пропускаем файлы, если они есть на верхнем уровне\n",
        "\n",
        "        # Ищем файлы train и test\n",
        "        train_files = []\n",
        "        test_file = None\n",
        "\n",
        "\n",
        "        train_files = [\n",
        "            os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.parquet') and 'train' in f\n",
        "        ]\n",
        "\n",
        "        test_file = next(\n",
        "            (os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.parquet') and 'test' in f),\n",
        "            None\n",
        "        )\n",
        "        print(train_files)\n",
        "        jamal = JAMAL(42)\n",
        "        y_pred = jamal.all_pipeline(train_path=train_files,\n",
        "                                   test_path=test_file)\n",
        "\n",
        "        del jamal\n",
        "        gc.collect()\n",
        "        predictions = pd.DataFrame()\n",
        "        predictions['id'] = pd.read_parquet(test_file)['id']\n",
        "        predictions['target'] = y_pred\n",
        "        predictions.to_csv(f\"predictions/{dataset_name}.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    make_predict()"
      ]
    }
  ]
}